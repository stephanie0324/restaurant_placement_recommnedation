{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incomplete-capability",
   "metadata": {},
   "source": [
    "# Create the features DF\n",
    "* using by_postal_code dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-examination",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "maritime-mother",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T01:42:51.786975Z",
     "start_time": "2023-05-18T01:42:50.640391Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import math \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "capital-trigger",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T01:42:51.800111Z",
     "start_time": "2023-05-18T01:42:51.797996Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "architectural-recognition",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T01:44:38.344276Z",
     "start_time": "2023-05-18T01:44:09.385945Z"
    }
   },
   "outputs": [],
   "source": [
    "# 導入資料\n",
    "train_df = pd.read_pickle('../data/Train_by_postoal_code_without_review_pointwise_v3_4.pkl').reset_index(drop=True)\n",
    "test_df = pd.read_pickle('../data/Test_by_postoal_code_without_review_pointwise_v3_4.pkl').reset_index(drop=True)\n",
    "all_df = pd.read_pickle('../Data/restaurant_only_s_with_embedding.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-bradley",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T01:44:38.947777Z",
     "start_time": "2023-05-18T01:44:38.944309Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_df.shape, test_df.shape, all_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d873bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-triumph",
   "metadata": {},
   "source": [
    "## Concat by restaurant and location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-psychology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T01:53:46.866379Z",
     "start_time": "2023-05-18T01:53:46.856823Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example list of sets\n",
    "# sets_list = list(train_df[:5].LDA_res)\n",
    "# combine_sets(sets_list)\n",
    "\n",
    "def combine_LDA_sets(sets_list):\n",
    "    # Create an empty dictionary to store the combined values\n",
    "    combined_values = {}\n",
    "\n",
    "    # Iterate over each set in the list\n",
    "    for s in sets_list:\n",
    "        # Iterate over each key-value pair in the set\n",
    "        for key, value in s:\n",
    "            # Check if the key already exists in the dictionary\n",
    "            if key in combined_values:\n",
    "                # Add the value to the existing key\n",
    "                combined_values[key] = torch.stack([combined_values[key] , value])\n",
    "            else:\n",
    "                # Add a new key-value pair to the dictionary\n",
    "                combined_values[key] = value\n",
    "    # Create a set from the combined key-value pairs\n",
    "    # combined_set = set(combined_values.items())\n",
    "\n",
    "    for key , val in combined_values.items():\n",
    "        combined_values[key] = np.mean(val)\n",
    "        \n",
    "    return combined_values\n",
    "\n",
    "########## GET LDA \n",
    "\n",
    "def get_LDA_aspects(df , all_df):\n",
    "\n",
    "    df['LDA_res'] = ''\n",
    "    df['LDA_loc'] = ''\n",
    "\n",
    "    for idx , row in df.iterrows():\n",
    "        if idx % 1000 ==0 :\n",
    "            print(f'Now progress... {idx}')\n",
    "        res = all_df[all_df.name == df.name[idx]]\n",
    "        res = res[res.postal_code != row.postal_code]\n",
    "        loc = all_df[all_df.postal_code == row.postal_code]\n",
    "        loc = loc[loc.name != df.name[idx]]\n",
    "        \n",
    "        res_sets_list = list(res.LDA_aspects)\n",
    "        loc_sets_list = list(loc.LDA_aspects)\n",
    "        \n",
    "        df['LDA_res'][idx] = combine_sets(res_sets_list)\n",
    "        df['LDA_loc'][idx] = combine_sets(loc_sets_list)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-nicholas",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:49:26.544922Z",
     "start_time": "2023-05-14T16:16:51.164897Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = get_LDA_aspects(train_df , all_df)\n",
    "test_df = get_LDA_aspects(test_df , all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b717b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example list of sets\n",
    "# sets_list = list(train_df[:5].LDA_res)\n",
    "# combine_sets(sets_list)\n",
    "\n",
    "def combine_senti_sets(sets_list):\n",
    "    # Create an empty dictionary to store the combined values\n",
    "    combined_values = {}\n",
    "    # Iterate over each set in the list\n",
    "    for s in sets_list:\n",
    "        # Iterate over each key-value pair in the set\n",
    "        for key, value in s.items():\n",
    "            \n",
    "            # Check if the key already exists in the dictionary\n",
    "            if key in combined_values:\n",
    "                combined_values[key].append(value)\n",
    "            else:\n",
    "                # Add a new key-value pair to the dictionary\n",
    "                combined_values[key] = []\n",
    "                combined_values[key].append(value)\n",
    "    # Create a set from the combined key-value pairs\n",
    "    # combined_set = set(combined_values.items())\n",
    "\n",
    "    for key , val in combined_values.items():\n",
    "        combined_values[key] =  torch.mean(torch.stack(val), dim=0)\n",
    "    return combined_values\n",
    "\n",
    "########## GET LDA \n",
    "\n",
    "def get_LDA_senti(df , all_df):\n",
    "\n",
    "    df['LDA_res_senti'] = ''\n",
    "    df['LDA_loc_senti'] = ''\n",
    "\n",
    "    for idx , row in df.iterrows():\n",
    "        if idx % 1000 ==0 :\n",
    "            print(f'Now progress... {idx}')\n",
    "        res = all_df[all_df.name == df.name[idx]]\n",
    "        res = res[res.postal_code != row.postal_code]\n",
    "        loc = all_df[all_df.postal_code == row.postal_code]\n",
    "        loc = loc[loc.name != df.name[idx]]\n",
    "        \n",
    "        res_sets_list = list(res.LDA_senti)\n",
    "        loc_sets_list = list(loc.LDA_senti)\n",
    "        \n",
    "        df['LDA_res_senti'][idx] = combine_senti_sets(res_sets_list)\n",
    "        df['LDA_loc_senti'][idx] = combine_senti_sets(loc_sets_list)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ea910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_LDA_senti(train_df , all_df)\n",
    "test_df = get_LDA_senti(test_df , all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "micro-stress",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T05:15:34.473149Z",
     "start_time": "2023-05-18T05:15:34.465630Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine_bert_embedding(df , all_df , col_name):\n",
    "    \n",
    "    df['res_'+str(col_name)] = ''\n",
    "    df['loc_'+str(col_name)] = ''\n",
    "    \n",
    "    for idx , row in df.iterrows():\n",
    "        \n",
    "        if idx % 1000 ==0 :\n",
    "            print(f'Now progress... {idx}')\n",
    "        \n",
    "        res = all_df[all_df.name == df.name[idx]]\n",
    "        res = res[res.postal_code != row.postal_code]\n",
    "        loc = all_df[all_df.postal_code == row.postal_code]\n",
    "        loc = loc[loc.name != df.name[idx]]\n",
    "\n",
    "        res_embedding = list(res[col_name])\n",
    "        loc_embedding = list(loc[col_name])\n",
    "        res_combined = []\n",
    "        loc_combined = []\n",
    "        for res_emb in res_embedding:\n",
    "            res_combined.append(torch.mean(torch.stack(res_emb[0]), dim=0))\n",
    "        for loc_emb in loc_embedding:\n",
    "            loc_combined.append(torch.mean(torch.stack(loc_emb[0]), dim=0))\n",
    "        df['res_'+str(col_name)][idx] = res_combined\n",
    "        df['loc_'+str(col_name)][idx] = loc_combined\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5089428e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'stars_x', 'useful', 'funny', 'cool', 'text', 'date',\n",
       "       'name', 'address', 'city', 'state', 'postal_code', 'latitude',\n",
       "       'longitude', 'stars_y', 'review_count', 'is_open', 'attributes',\n",
       "       'categories', 'hours', 'LDA_aspects', 'BERT_embedding', 'LDA_senti',\n",
       "       'self_senti'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7eab6c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now progress... 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/emma/bilab/Steph_C/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/mnt/data/emma/bilab/Steph_C/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-2c404b8be186>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombine_bert_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mall_df\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m'BERT_embedding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-102-dabcaa8c2061>\u001b[0m in \u001b[0;36mcombine_bert_embedding\u001b[0;34m(df, all_df, col_name)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloc_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mres_emb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres_embedding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mres_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mloc_emb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloc_embedding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloc_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "print(combine_bert_embedding(train_df[:1] , all_df ,'BERT_embedding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-chorus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T17:18:10.572755Z",
     "start_time": "2023-05-14T16:49:26.556691Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = combine_bert_embedding(train_df , all_df)\n",
    "test_df = combine_bert_embedding(test_df , all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-worry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T04:40:35.002143Z",
     "start_time": "2023-05-18T04:40:34.755537Z"
    }
   },
   "outputs": [],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_senti(df):\n",
    "    \n",
    "    df['res_emb'] = ''\n",
    "    df['loc_emb'] = ''\n",
    "    \n",
    "    for idx , row in df.iterrows():\n",
    "        \n",
    "        if idx % 1000 ==0 :\n",
    "            print(f'Now progress... {idx}')\n",
    "        \n",
    "        res = all_df[all_df.name == df.name[idx]]\n",
    "        res = res[res.postal_code != row.postal_code]\n",
    "        loc = all_df[all_df.postal_code == row.postal_code]\n",
    "        loc = loc[loc.name != df.name[idx]]\n",
    "        \n",
    "        res_embedding = list(res.BERT_embedding)\n",
    "        loc_embedding = list(loc.BERT_embedding)\n",
    "        df['res_emb'][idx] = torch.mean(torch.stack(res_embedding), dim=0)\n",
    "        df['loc_emb'][idx] = torch.mean(torch.stack(loc_embedding), dim=0)\n",
    "        \n",
    "        \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-mystery",
   "metadata": {},
   "source": [
    "## Affinity and Complementary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-romance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T17:18:10.577786Z",
     "start_time": "2023-05-14T17:18:10.573933Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_affinity_complementary(df):\n",
    "    df['affinity'] = ''\n",
    "    df['complementary'] = ''\n",
    "    \n",
    "    for idx , row in df.iterrows():\n",
    "        affinity , complementary = 0,0\n",
    "        if idx % 1000 ==0 :\n",
    "            print(f'Now progress... {idx}')\n",
    "        for res_key , res_val in df.LDA_res[idx].items():\n",
    "            if res_key in df.LDA_loc[0]:\n",
    "                affinity += res_val*train_df.LDA_loc[0][res_key]\n",
    "                complementary += res_val*(1-train_df.LDA_loc[0][res_key])\n",
    "        df['affinity'][idx] = affinity\n",
    "        df['complementary'][idx] = complementary\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-description",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T17:18:48.240238Z",
     "start_time": "2023-05-14T17:18:10.578879Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = get_affinity_complementary(train_df)\n",
    "test_df = get_affinity_complementary(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-vertex",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T17:18:48.246025Z",
     "start_time": "2023-05-14T17:18:48.241733Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_cosine_similarity(df):\n",
    "    df['cosine_sim'] = ''\n",
    "    \n",
    "    for idx , row in df.iterrows():\n",
    "        if idx % 1000 ==0 :\n",
    "            print(f'Now progress... {idx}')\n",
    "        df['cosine_sim'][idx]= F.cosine_similarity(row.res_emb.unsqueeze(0), row.loc_emb.unsqueeze(0))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-admission",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T17:18:53.455514Z",
     "start_time": "2023-05-14T17:18:48.247323Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = get_cosine_similarity(train_df)\n",
    "test_df = get_cosine_similarity(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LDA_senti_score(df):\n",
    "    \n",
    "    df['LDA_senti_score'] = ''\n",
    "    pos_neg_list = [-1,0,1]\n",
    "    \n",
    "    for idx , row in df.iterrows():\n",
    "        senti_score = 0\n",
    "        if idx % 1000 ==0 :\n",
    "            print(f'Now progress... {idx}')\n",
    "        for res_key , res_val in df.LDA_res[idx].items():\n",
    "            if res_key in df.LDA_loc[0]:\n",
    "                res_sentiment = pos_neg_list[np.argmax(df.LDA_senti[idx][res_key])]\n",
    "                affinity += res_val**train_df.LDA_loc[0][res_key]\n",
    "                complementary += res_val*(1-train_df.LDA_loc[0][res_key])\n",
    "        df['affinity'][idx] = affinity\n",
    "        df['complementary'][idx] = complementary\n",
    "        \n",
    "    return df\n",
    "    for idx , row in df.iterrows:\n",
    "        if idx % 1000 ==0:\n",
    "            print(f'Now progress ... {idx}')\n",
    "        for res_key , res_val in df.LDA_res[idx].items():\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_self_senti_score(df):\n",
    "    \n",
    "    df['self_senti_score'] = ''\n",
    "    pos_neg_list = [-1,0,1]\n",
    "    \n",
    "    for idx , row in df.iterrows():\n",
    "        senti_score = 0\n",
    "        if idx % 1000 ==0 :\n",
    "            print(f'Now progress... {idx}')\n",
    "        for res_key , res_val in df.LDA_res[idx].items():\n",
    "            if res_key in df.LDA_loc[0]:\n",
    "                affinity += res_val*train_df.LDA_loc[0][res_key]\n",
    "                complementary += res_val*(1-train_df.LDA_loc[0][res_key])\n",
    "        df['affinity'][idx] = affinity\n",
    "        df['complementary'][idx] = complementary\n",
    "        \n",
    "    return df\n",
    "    for idx , row in df.iterrows:\n",
    "        if idx % 1000 ==0:\n",
    "            print(f'Now progress ... {idx}')\n",
    "        for res_key , res_val in df.LDA_res[idx].items():\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-bottle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T17:19:01.559545Z",
     "start_time": "2023-05-14T17:18:53.457059Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.to_pickle('../data/Train_by_postoal_code_pointwise_v3_4.pkl')\n",
    "test_df.to_pickle('../data/Test_by_postoal_code_pointwise_v3_4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-medication",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s_env",
   "language": "python",
   "name": "s_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f457e4bdb0a834eef26de468bbb8aa330d736c8b7db558705075494c4e15f1a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
