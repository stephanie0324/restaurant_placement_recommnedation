{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966237c4",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "588548e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T15:48:18.796352Z",
     "start_time": "2023-05-02T15:48:18.463623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Tue May  2 23:48:18 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| 41%   51C    P0    55W / 260W |     37MiB / 11019MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n",
      "| 22%   28C    P8    10W / 260W |     12MiB / 11019MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1588      G   /usr/lib/xorg/Xorg                             9MiB |\r\n",
      "|    0      1953      G   /usr/bin/gnome-shell                          14MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c9ac79b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T01:28:01.044229Z",
     "start_time": "2023-05-03T01:28:01.037497Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "41363b5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T01:28:01.568948Z",
     "start_time": "2023-05-03T01:28:01.559426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb6b751e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T09:24:30.025445Z",
     "start_time": "2023-05-02T09:24:24.541692Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/.local/lib/python3.7/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2023-05-02 17:24:25.539204: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-05-02 17:24:28.110419: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2023-05-02 17:24:28.147032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-02 17:24:28.147338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2023-05-02 17:24:28.147365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-02 17:24:28.147645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:02:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2023-05-02 17:24:28.147660: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-05-02 17:24:28.147679: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2023-05-02 17:24:28.147690: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-05-02 17:24:28.187748: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2023-05-02 17:24:28.195743: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2023-05-02 17:24:28.196301: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-02 17:24:28.196795: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-02 17:24:28.196912: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-05-02 17:24:28.196940: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "280c882a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T09:24:34.466041Z",
     "start_time": "2023-05-02T09:24:34.399552Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('/home/adam/Steph_C/my_thesis/data/Train_by_postoal_code_without_review_pointwise_v3_3.pkl')\n",
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2314b2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## SPACY and TextBlob\n",
    "* [SPACY and TextBlob](https://towardsdatascience.com/aspect-based-sentiment-analysis-using-spacy-textblob-4c8de3e0d2b9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be1aa99f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T09:25:25.587251Z",
     "start_time": "2023-05-02T09:25:25.581912Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Counter(train_df.name)\n",
    "# Joe Boccardi's Ristorante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0f5911a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T09:25:26.015392Z",
     "start_time": "2023-05-02T09:25:26.008735Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = ['Mushrooms n calamari was not bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d68e2a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T09:25:26.460991Z",
     "start_time": "2023-05-02T09:25:26.443718Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tmp = train_df[train_df.name== 'Joe Boccardi\\'s Ristorante']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b308a43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T09:25:26.857027Z",
     "start_time": "2023-05-02T09:25:26.851558Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentences = list(tmp.text)\n",
    "sentences = list(dict.fromkeys(sentences)) # deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba37b8d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T09:25:27.748887Z",
     "start_time": "2023-05-02T09:25:27.740257Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the best italian restaurant andrea makes the best chicken ala mario. The house dressing is to die for.',\n",
       " \"Cajun pasta was YUM!!!! Mushrooms n calamari wasn't bad... waitress n staff were great...servings big def recommend!!\",\n",
       " 'Still our favorite anytime place, but they make Saturday date night even better with their seafood special linguini and regular chicken parm. Sooo good!!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d3fad7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T09:25:28.905266Z",
     "start_time": "2023-05-02T09:25:28.812169Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the best italian restaurant andrea makes the best chicken ala mario\n",
      " The house dressing is to die for\n",
      "\n",
      "Cajun pasta was YUM!!!! Mushrooms n calamari wasn't bad\n",
      "\n",
      "\n",
      " waitress n staff were great\n",
      "\n",
      "\n",
      "servings big def recommend!!\n",
      "Still our favorite anytime place, but they make Saturday date night even better with their seafood special linguini and regular chicken parm\n",
      " Sooo good!!\n",
      "[{'aspect': '', 'description': 'best'}, {'aspect': 'house', 'description': ''}, {'aspect': '', 'description': ''}, {'aspect': 'Mushrooms', 'description': 'bad'}, {'aspect': '', 'description': ''}, {'aspect': '', 'description': ''}, {'aspect': 'staff', 'description': 'great'}, {'aspect': '', 'description': ''}, {'aspect': '', 'description': ''}, {'aspect': '', 'description': 'big'}, {'aspect': '', 'description': 'regular'}, {'aspect': '', 'description': 'good'}]\n"
     ]
    }
   ],
   "source": [
    "aspects = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    for sent in sentence.split('.'):\n",
    "        doc = nlp(sent)\n",
    "        print(doc)\n",
    "        descriptive_term = ''\n",
    "        target = ''\n",
    "        for token in doc:\n",
    "            if token.dep_ == 'nsubj' and token.pos_ == 'NOUN':\n",
    "                target = token.text\n",
    "            if token.pos_ == 'ADJ':\n",
    "                prepend = ''\n",
    "                for child in token.children:\n",
    "                    if child.pos_ != 'ADV':\n",
    "                        continue\n",
    "                    prepend += child.text + ' '\n",
    "                descriptive_term = prepend + token.text\n",
    "        aspects.append({'aspect': target,'description': descriptive_term})\n",
    "\n",
    "print(aspects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e6271ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T09:25:30.528041Z",
     "start_time": "2023-05-02T09:25:29.546075Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'aspect': '', 'description': 'best', 'sentiment': 1.0}, {'aspect': 'house', 'description': '', 'sentiment': 0.0}, {'aspect': '', 'description': '', 'sentiment': 0.0}, {'aspect': 'Mushrooms', 'description': 'bad', 'sentiment': -0.6999999999999998}, {'aspect': '', 'description': '', 'sentiment': 0.0}, {'aspect': '', 'description': '', 'sentiment': 0.0}, {'aspect': 'staff', 'description': 'great', 'sentiment': 0.8}, {'aspect': '', 'description': '', 'sentiment': 0.0}, {'aspect': '', 'description': '', 'sentiment': 0.0}, {'aspect': '', 'description': 'big', 'sentiment': 0.0}, {'aspect': '', 'description': 'regular', 'sentiment': 0.0}, {'aspect': '', 'description': 'good', 'sentiment': 0.7}]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "for aspect in aspects:\n",
    "    aspect['sentiment'] = TextBlob(aspect['description']).sentiment.polarity\n",
    "print(aspects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d806b679",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## aspect-based-sentiment-analysis 2.0.3\n",
    "* can only work on google colab\n",
    "* [python package](https://pypi.org/project/aspect-based-sentiment-analysis/)\n",
    "* [example from youtube](https://www.youtube.com/watch?v=q8sTicXK4Fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5805a792",
   "metadata": {},
   "source": [
    "## VaderSentiment.vaderSentiment\n",
    "* [Kaggle notebook](https://www.kaggle.com/code/phiitm/aspect-based-sentiment-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee829464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T09:25:31.587183Z",
     "start_time": "2023-05-02T09:25:31.568498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER is smart, handsome, and funny.----------------------------- {'neg': 0.0, 'neu': 0.254, 'pos': 0.746, 'compound': 0.8316}\n",
      "VADER is smart, handsome, and funny!----------------------------- {'neg': 0.0, 'neu': 0.248, 'pos': 0.752, 'compound': 0.8439}\n",
      "VADER is very smart, handsome, and funny.------------------------ {'neg': 0.0, 'neu': 0.299, 'pos': 0.701, 'compound': 0.8545}\n",
      "VADER is VERY SMART, handsome, and FUNNY.------------------------ {'neg': 0.0, 'neu': 0.246, 'pos': 0.754, 'compound': 0.9227}\n",
      "VADER is VERY SMART, handsome, and FUNNY!!!---------------------- {'neg': 0.0, 'neu': 0.233, 'pos': 0.767, 'compound': 0.9342}\n",
      "VADER is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!--------- {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.9469}\n",
      "VADER is not smart, handsome, nor funny.------------------------- {'neg': 0.646, 'neu': 0.354, 'pos': 0.0, 'compound': -0.7424}\n",
      "The book was good.----------------------------------------------- {'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}\n",
      "At least it isn't a horrible book.------------------------------- {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.431}\n",
      "The book was only kind of good.---------------------------------- {'neg': 0.0, 'neu': 0.697, 'pos': 0.303, 'compound': 0.3832}\n",
      "The plot was good, but the characters are uncompelling and the dialog is not great. {'neg': 0.327, 'neu': 0.579, 'pos': 0.094, 'compound': -0.7042}\n",
      "Today SUX!------------------------------------------------------- {'neg': 0.779, 'neu': 0.221, 'pos': 0.0, 'compound': -0.5461}\n",
      "Today only kinda sux! But I'll get by, lol----------------------- {'neg': 0.127, 'neu': 0.556, 'pos': 0.317, 'compound': 0.5249}\n",
      "Make sure you :) or :D today!------------------------------------ {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.8633}\n",
      "Catch utf-8 emoji such as such as 💘 and 💋 and 😁------------------ {'neg': 0.0, 'neu': 0.615, 'pos': 0.385, 'compound': 0.875}\n",
      "Not bad at all--------------------------------------------------- {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.431}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "#note: depending on how you installed (e.g., using source code download versus pip install), you may need to import like this:\n",
    "#from vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# --- examples -------\n",
    "sentences = [\"VADER is smart, handsome, and funny.\",  # positive sentence example\n",
    "             \"VADER is smart, handsome, and funny!\",  # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "             \"VADER is very smart, handsome, and funny.\", # booster words handled correctly (sentiment intensity adjusted)\n",
    "             \"VADER is VERY SMART, handsome, and FUNNY.\",  # emphasis for ALLCAPS handled\n",
    "             \"VADER is VERY SMART, handsome, and FUNNY!!!\", # combination of signals - VADER appropriately adjusts intensity\n",
    "             \"VADER is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!\", # booster words & punctuation make this close to ceiling for score\n",
    "             \"VADER is not smart, handsome, nor funny.\",  # negation sentence example\n",
    "             \"The book was good.\",  # positive sentence\n",
    "             \"At least it isn't a horrible book.\",  # negated negative sentence with contraction\n",
    "             \"The book was only kind of good.\", # qualified positive sentence is handled correctly (intensity adjusted)\n",
    "             \"The plot was good, but the characters are uncompelling and the dialog is not great.\", # mixed negation sentence\n",
    "             \"Today SUX!\",  # negative slang with capitalization emphasis\n",
    "             \"Today only kinda sux! But I'll get by, lol\", # mixed sentiment example with slang and constrastive conjunction \"but\"\n",
    "             \"Make sure you :) or :D today!\",  # emoticons handled\n",
    "             \"Catch utf-8 emoji such as such as 💘 and 💋 and 😁\",  # emojis handled\n",
    "             \"Not bad at all\"  # Capitalized negation\n",
    "             ]\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<65} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a15c59",
   "metadata": {},
   "source": [
    "## HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd93b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, DebertaV2ForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge\")\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(\"microsoft/deberta-v2-xlarge\", problem_type=\"multi_label_classification\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
    "\n",
    "# To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
    "num_labels = len(model.config.id2label)\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v2-xlarge\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "36232a22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T10:28:41.971691Z",
     "start_time": "2023-05-02T10:28:36.795620Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2573, -0.0185]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1837/331051848.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Interpret the sentiment result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_class_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input Text: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, DebertaV2ForSequenceClassification\n",
    "\n",
    "sentiment_labels = [\"Positive\", \"Neutral\",\"Negative\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge\")\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(\"microsoft/deberta-v2-xlarge\")\n",
    "model.eval()\n",
    "\n",
    "input_text = \"Hello, my dog is ugly\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0)>0.5]\n",
    "print(logits)\n",
    "\n",
    "# Interpret the sentiment result\n",
    "sentiment = sentiment_labels[predicted_class_ids]\n",
    "\n",
    "print(\"Input Text: \", input_text)\n",
    "print(\"Predicted Sentiment: \", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5d811fc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T15:49:59.192638Z",
     "start_time": "2023-05-02T15:49:53.845190Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:  The battery life of this laptop is sooo bad.\n",
      "Aspect:  battery life\n",
      "Predicted Sentiment:  Positive\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DebertaV2ForSequenceClassification, DebertaV2Tokenizer\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "model_name = \"microsoft/deberta-v2-xlarge\"  # Pretrained DeBERTa-v2 model\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(model_name)\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Set the device for inference\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Example input text and aspect\n",
    "input_text = \"The battery life of this laptop is sooo bad.\"\n",
    "aspect = \"battery life\"\n",
    "\n",
    "# Combine the input text and aspect\n",
    "input_text_with_aspect = f\"{input_text} [ASP] {aspect}\"\n",
    "\n",
    "# Tokenize the input text with aspect\n",
    "encoded_input = tokenizer.encode_plus(input_text_with_aspect, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "input_ids = encoded_input[\"input_ids\"].to(device)\n",
    "attention_mask = encoded_input[\"attention_mask\"].to(device)\n",
    "\n",
    "# Perform aspect sentiment analysis inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get the predicted sentiment label\n",
    "predicted_sentiment = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# Interpret the sentiment result\n",
    "sentiment_labels = [\"Positive\", \"Neutral\", \"Negative\"]\n",
    "sentiment = sentiment_labels[predicted_sentiment]\n",
    "\n",
    "print(\"Input Text: \", input_text)\n",
    "print(\"Aspect: \", aspect)\n",
    "print(\"Predicted Sentiment: \", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f27ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steph-env",
   "language": "python",
   "name": "steph-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
