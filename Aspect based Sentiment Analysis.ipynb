{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "completed-nebraska",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fourth-beginning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T03:35:19.652451Z",
     "start_time": "2023-05-10T03:35:19.143074Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "magnetic-tuition",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T03:35:25.308301Z",
     "start_time": "2023-05-10T03:35:25.304300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cross-sheep",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:53:18.059128Z",
     "start_time": "2023-05-03T03:53:18.030569Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('../data/Train_by_postoal_code_without_review_pointwise_v3_3.pkl').reset_index(drop=True)\n",
    "test_df = pd.read_pickle('../data/Test_by_postoal_code_without_review_pointwise_v3_3.pkl').reset_index(drop=True)\n",
    "all_df = pd.read_pickle('../Data/restaurant_only.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "excited-sample",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:53:19.803443Z",
     "start_time": "2023-05-03T03:53:19.031617Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8c262a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1136, 712)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Counter(train_df.business_id)) , len(Counter(test_df.business_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2422e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text\n",
    "for idx , row in train_df.iterrows():\n",
    "    res = all_df[all_df.business_id == row.business_id].sort_values(by='date',ascending=False)\n",
    "    loc = all_df[all_df.postal_code == row.postal_code].sort_values(by='date',ascending=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-aggregate",
   "metadata": {},
   "source": [
    "## SPACY and TextBlob\n",
    "* [SPACY and TextBlob](https://towardsdatascience.com/aspect-based-sentiment-analysis-using-spacy-textblob-4c8de3e0d2b9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-flesh",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:53:22.639097Z",
     "start_time": "2023-05-03T03:53:22.633858Z"
    }
   },
   "outputs": [],
   "source": [
    "# Counter(train_df.name)\n",
    "# Joe Boccardi's Ristorante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-deadline",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:53:23.343325Z",
     "start_time": "2023-05-03T03:53:23.337200Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = ['Mushrooms n calamari was not bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-cisco",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:53:23.866598Z",
     "start_time": "2023-05-03T03:53:23.861913Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp = train_df[train_df.name== 'Joe Boccardi\\'s Ristorante']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-purchase",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:53:24.287339Z",
     "start_time": "2023-05-03T03:53:24.282012Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = list(tmp.text)\n",
    "sentences = list(dict.fromkeys(sentences)) # deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-walter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:53:24.604014Z",
     "start_time": "2023-05-03T03:53:24.594729Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-rendering",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:53:24.924196Z",
     "start_time": "2023-05-03T03:53:24.875913Z"
    }
   },
   "outputs": [],
   "source": [
    "aspects = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    for sent in sentence.split('.'):\n",
    "        doc = nlp(sent)\n",
    "        print(doc)\n",
    "        descriptive_term = ''\n",
    "        target = ''\n",
    "        for token in doc:\n",
    "            if token.dep_ == 'nsubj' and token.pos_ == 'NOUN':\n",
    "                target = token.text\n",
    "            if token.pos_ == 'ADJ':\n",
    "                prepend = ''\n",
    "                for child in token.children:\n",
    "                    if child.pos_ != 'ADV':\n",
    "                        continue\n",
    "                    prepend += child.text + ' '\n",
    "                descriptive_term = prepend + token.text\n",
    "        aspects.append({'aspect': target,'description': descriptive_term})\n",
    "\n",
    "print(aspects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-blank",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:53:29.662298Z",
     "start_time": "2023-05-03T03:53:28.878527Z"
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "for aspect in aspects:\n",
    "    aspect['sentiment'] = TextBlob(aspect['description']).sentiment.polarity\n",
    "print(aspects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-martial",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## aspect-based-sentiment-analysis 2.0.3\n",
    "* can only work on google colab\n",
    "* [python package](https://pypi.org/project/aspect-based-sentiment-analysis/)\n",
    "* [example from youtube](https://www.youtube.com/watch?v=q8sTicXK4Fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-figure",
   "metadata": {},
   "source": [
    "## VaderSentiment.vaderSentiment\n",
    "* [Kaggle notebook](https://www.kaggle.com/code/phiitm/aspect-based-sentiment-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-ownership",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:53:32.772452Z",
     "start_time": "2023-05-03T03:53:32.751815Z"
    }
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "#note: depending on how you installed (e.g., using source code download versus pip install), you may need to import like this:\n",
    "#from vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# --- examples -------\n",
    "sentences = [\"VADER is smart, handsome, and funny.\",  # positive sentence example\n",
    "             \"VADER is smart, handsome, and funny!\",  # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "             \"VADER is very smart, handsome, and funny.\", # booster words handled correctly (sentiment intensity adjusted)\n",
    "             \"VADER is VERY SMART, handsome, and FUNNY.\",  # emphasis for ALLCAPS handled\n",
    "             \"VADER is VERY SMART, handsome, and FUNNY!!!\", # combination of signals - VADER appropriately adjusts intensity\n",
    "             \"VADER is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!\", # booster words & punctuation make this close to ceiling for score\n",
    "             \"VADER is not smart, handsome, nor funny.\",  # negation sentence example\n",
    "             \"The book was good.\",  # positive sentence\n",
    "             \"At least it isn't a horrible book.\",  # negated negative sentence with contraction\n",
    "             \"The book was only kind of good.\", # qualified positive sentence is handled correctly (intensity adjusted)\n",
    "             \"The plot was good, but the characters are uncompelling and the dialog is not great.\", # mixed negation sentence\n",
    "             \"Today SUX!\",  # negative slang with capitalization emphasis\n",
    "             \"Today only kinda sux! But I'll get by, lol\", # mixed sentiment example with slang and constrastive conjunction \"but\"\n",
    "             \"Make sure you :) or :D today!\",  # emoticons handled\n",
    "             \"Catch utf-8 emoji such as such as üíò and üíã and üòÅ\",  # emojis handled\n",
    "             \"Not bad at all\"  # Capitalized negation\n",
    "             ]\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<65} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-portfolio",
   "metadata": {},
   "source": [
    "## HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-ribbon",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-10T03:38:36.875Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/emma/bilab/Steph_C/steph_env/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"yangheng/deberta-v3-large-absa-v1.1\")\n",
    "\n",
    "inputs = tokenizer(\"[CLS] when tables opened up, the manager sat another party before us. [SEP] manager [SEP]\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-elite",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:53:38.642717Z",
     "start_time": "2023-05-03T03:53:38.592780Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, DebertaV2ForSequenceClassification\n",
    "\n",
    "sentiment_labels = [\"Positive\", \"Neutral\",\"Negative\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v2-xlarge\")\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(\"microsoft/deberta-v2-xlarge\")\n",
    "model.eval()\n",
    "\n",
    "input_text = \"Hello, my dog is ugly\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0)>0.5]\n",
    "print(logits)\n",
    "\n",
    "# Interpret the sentiment result\n",
    "sentiment = sentiment_labels[predicted_class_ids]\n",
    "\n",
    "print(\"Input Text: \", input_text)\n",
    "print(\"Predicted Sentiment: \", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-antibody",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:53:43.725754Z",
     "start_time": "2023-05-03T03:53:43.722938Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp = train_df[train_df.name == 'Cafe Patachou']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-quarterly",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:53:44.077059Z",
     "start_time": "2023-05-03T03:53:44.074656Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = list(tmp.text)\n",
    "sentences = list(dict.fromkeys(sentences)) # deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-disclaimer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:53:44.419136Z",
     "start_time": "2023-05-03T03:53:44.410965Z"
    }
   },
   "outputs": [],
   "source": [
    "Counter(train_df.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-receptor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:53:44.897840Z",
     "start_time": "2023-05-03T03:53:44.769701Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DebertaV2ForSequenceClassification, DebertaV2Tokenizer\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "model_name = \"microsoft/deberta-v2-xlarge\"  # Pretrained DeBERTa-v2 model\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(model_name)\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Set the device for inference\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Example input text and aspect\n",
    "input_text = sentences[0]\n",
    "aspect = \"Food\"\n",
    "\n",
    "def prediction(input_text , aspect):\n",
    "    # Combine the input text and aspect\n",
    "    input_text_with_aspect = f\"{input_text} [ASP] {aspect}\"\n",
    "\n",
    "    # Tokenize the input text with aspect\n",
    "    encoded_input = tokenizer.encode_plus(input_text_with_aspect, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = encoded_input[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded_input[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Perform aspect sentiment analysis inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Get the predicted sentiment label\n",
    "    predicted_sentiment = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Interpret the sentiment result\n",
    "    sentiment_labels = [\"Positive\", \"Neutral\", \"Negative\"]\n",
    "    sentiment = sentiment_labels[predicted_sentiment]\n",
    "\n",
    "    print(\"Input Text: \", input_text)\n",
    "    print(\"Aspect: \", aspect)\n",
    "    print(\"Predicted Sentiment: \", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-rider",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T03:53:45.623082Z",
     "start_time": "2023-05-03T03:53:45.612811Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in sentences:\n",
    "    prediction(i , 'food')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-reproduction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f457e4bdb0a834eef26de468bbb8aa330d736c8b7db558705075494c4e15f1a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
