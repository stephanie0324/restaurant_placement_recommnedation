{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exceptional-roads",
   "metadata": {},
   "source": [
    "# Pointwise\n",
    "* predict the relevance scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-handbook",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "previous-leather",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:58:44.267050Z",
     "start_time": "2023-05-12T05:58:44.260409Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# models\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from lightgbm import LGBMRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "convenient-walter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:58:44.684637Z",
     "start_time": "2023-05-12T05:58:44.681761Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "synthetic-wagon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:58:45.191606Z",
     "start_time": "2023-05-12T05:58:45.181027Z"
    }
   },
   "outputs": [],
   "source": [
    "# 導入資料\n",
    "train_df = pd.read_pickle('../data/Train_by_postoal_code_without_review_pointwise_v3_3.pkl')\n",
    "test_df = pd.read_pickle('../data/Test_by_postoal_code_without_review_pointwise_v3_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "seven-winning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:59:46.391564Z",
     "start_time": "2023-05-12T05:59:44.144005Z"
    }
   },
   "outputs": [],
   "source": [
    "# expr. 1 change relevance score to binary\n",
    "def change_rel_score(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        if row.relevance > 0:\n",
    "            row.relevance = 1\n",
    "        else:\n",
    "            pass\n",
    "    return df\n",
    "train_df = change_rel_score(train_df)\n",
    "test_df = change_rel_score(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dimensional-colleague",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:52:29.411705Z",
     "start_time": "2023-05-12T05:52:29.403316Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1 , random_state = RANDOM_STATE)\n",
    "test_df = test_df.sample(frac=1 , random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "transparent-clear",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:52:29.489360Z",
     "start_time": "2023-05-12T05:52:29.415520Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Counter(train_df.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "consecutive-blond",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:52:29.576813Z",
     "start_time": "2023-05-12T05:52:29.493908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11324, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "handled-rocket",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:52:29.663476Z",
     "start_time": "2023-05-12T05:52:29.580815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7098, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-underground",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "relevant-frame",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:52:29.746197Z",
     "start_time": "2023-05-12T05:52:29.672625Z"
    }
   },
   "outputs": [],
   "source": [
    "def _precision(predictions , actuals, k = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the precision at k\n",
    "    \n",
    "    Returns: a list of precisions\n",
    "    \"\"\"\n",
    "    \n",
    "    precisions =[]\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        \n",
    "        prediction = predictions[i]\n",
    "\n",
    "        if  k != None:\n",
    "            prediction =  predictions[i][:k]\n",
    "        \n",
    "        score = 0\n",
    "        for j in prediction:\n",
    "            if j in actuals[i]:\n",
    "                score+=1\n",
    "        if len(prediction) != 0:\n",
    "            precisions.append(score/len(prediction))\n",
    "        else:\n",
    "            precisions.append(0)\n",
    "    return precisions\n",
    "    \n",
    "\n",
    "def _recall(predictions , actuals, k = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the precision at k\n",
    "    \n",
    "    Returns: a list of recalls\n",
    "    \"\"\"\n",
    "    recalls =[]\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        \n",
    "        prediction =  predictions[i]\n",
    "        \n",
    "        if  k != None:\n",
    "            prediction =  predictions[i][:k]\n",
    "        \n",
    "        score = 0\n",
    "        for j in range(len(prediction)):\n",
    "            if prediction[j] in actuals[i]:\n",
    "                score+=1\n",
    "        recalls.append(score/len(actuals[i]))\n",
    "    \n",
    "    return recalls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "herbal-cigarette",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:52:29.829376Z",
     "start_time": "2023-05-12T05:52:29.751158Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_mrr(predictions, actuals):\n",
    "    \"\"\"\n",
    "    Calculate the mean reciprocal rank (MRR) for a set of predictions and actual values.\n",
    "    \n",
    "    Parameters:\n",
    "    predictions (list of lists): A list of predicted rankings sorted by probability.\n",
    "    actual (list of lists): A list of actual rankings sorted by relevance.\n",
    "    \n",
    "    Returns:    \n",
    "    float: A list of MRR scores.\n",
    "    \"\"\"\n",
    "    mrr_list = []\n",
    "    for i in range(len(predictions)):\n",
    "        reciprocal_rank = 0\n",
    "        if actuals[i][0] in predictions[i]:\n",
    "            reciprocal_rank = 1/ (predictions[i].index(actuals[i][0]) + 1)\n",
    "        mrr_list.append(reciprocal_rank)\n",
    "    return mrr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "conscious-highlight",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:52:29.945093Z",
     "start_time": "2023-05-12T05:52:29.837197Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_map( predictions , actuals, k=None):\n",
    "    \"\"\"\n",
    "    Calculate the mean average precision (MAP) for a set of queries.\n",
    "\n",
    "    Parameters:\n",
    "    actual (list of sets or lists): A list of sets or lists of the actual relevant items for each query.\n",
    "    predicted (list of lists): A list of lists of predicted items for each query.\n",
    "    k (int): The maximum number of predicted items to consider for each query.\n",
    "\n",
    "    Returns:\n",
    "    float: A list of MAP scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    map_list = []\n",
    "    \n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        \n",
    "        ap_list = []\n",
    "        hit = 0 \n",
    "        cnt = 0 \n",
    "        \n",
    "        prediction =  predictions[i]\n",
    "        \n",
    "        if k != None:\n",
    "            prediction =  predictions[i][:k]\n",
    "        \n",
    "        \n",
    "        for j in prediction:\n",
    "            if j in actuals[i]:\n",
    "                hit+=1\n",
    "                cnt+=1\n",
    "                ap_list.append(hit/cnt)\n",
    "            else:\n",
    "                cnt+=1\n",
    "        if len(ap_list) != 0:\n",
    "            map_list.append(np.mean(ap_list))\n",
    "        else:\n",
    "            map_list.append(0)\n",
    "    \n",
    "    return map_list\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "backed-biography",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:52:30.033756Z",
     "start_time": "2023-05-12T05:52:29.954303Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_dcg_ndcg( predictions , actuals, rel ,k=None):\n",
    "    \"\"\"\n",
    "    Calculate the DCG@k , NDCG@k for a set of queries.\n",
    "\n",
    "    Parameters:\n",
    "    actual (list of sets or lists): A list of sets or lists of the actual relevant items for each query.\n",
    "    predicted (list of lists): A list of lists of predicted items for each query.\n",
    "    k (int): The maximum number of predicted items to consider for each query.\n",
    "\n",
    "    Returns:\n",
    "    float: A list of DCG , NDCG scores.\n",
    "    \"\"\"\n",
    "    dcg_list = []\n",
    "    ndcg_list = []\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        dcg =0\n",
    "        idcg =0\n",
    "        \n",
    "        prediction = predictions[i]\n",
    "        \n",
    "        if k != None:\n",
    "            prediction = predictions[i][:k]\n",
    "        \n",
    "        for j in range(len(actuals[i])):\n",
    "            if actuals[i][j] in prediction:\n",
    "                rank = prediction.index(actuals[i][j]) + 1\n",
    "                dcg += np.divide(float(rel[i][j]),np.log2(rank+1))\n",
    "            idcg += np.divide(float(rel[i][j]),np.log2((j+1)+1))\n",
    "        dcg_list.append(dcg)\n",
    "        if np.divide(dcg,idcg) > 1:\n",
    "            print(rel[i], prediction,actuals[i]  )\n",
    "            print(i,dcg,idcg  , 'Wrong !!!')\n",
    "        ndcg_list.append(np.divide(dcg,idcg))\n",
    "        \n",
    "    return dcg_list , ndcg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "differential-roller",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:52:30.152485Z",
     "start_time": "2023-05-12T05:52:30.040007Z"
    }
   },
   "outputs": [],
   "source": [
    "# create list of list for query ranking\n",
    "def get_ranking(df ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Turn the probability array into a list of lists for calculation.\n",
    "    \n",
    "    Parameters:\n",
    "    df(DataFrame): the test dataframe\n",
    "    \n",
    "    Returns:\n",
    "    prediction (list of lists): A list of predicted rankings for each query.\n",
    "    actual (list of lists): A list of actual rankings for each query.\n",
    "    \"\"\"\n",
    " \n",
    "    pred_list = []\n",
    "    true_rel = []\n",
    "    true_list = []\n",
    "\n",
    "    output_dict = {}\n",
    "    \n",
    "    \n",
    "    for id in Counter(df.business_id):\n",
    "\n",
    "        output_dict[id] = {}\n",
    "        \n",
    "        tmp = df[df.business_id == id]\n",
    "        a_sorted = tmp.sort_values(by=['relevance'],ascending=[False])\n",
    "        p_sorted = tmp.sort_values(by=['predictions'],ascending=[False])\n",
    "        # p_sorted = p_sorted[p_sorted.predictions>0]\n",
    "\n",
    "        true_list.append(list(a_sorted[a_sorted.relevance!=0].postal_code))\n",
    "        pred_list.append(list(p_sorted.postal_code))\n",
    "        true_rel.append(list(a_sorted[a_sorted.relevance!=0].relevance))\n",
    "        \n",
    "        output_dict[id]['predict'] = list(p_sorted.postal_code)\n",
    "        output_dict[id]['true'] = list(a_sorted[a_sorted.relevance!=0].postal_code)\n",
    "        \n",
    "        \n",
    "    return pred_list, true_rel , true_list , output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-sight",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cheap-entry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:52:30.317524Z",
     "start_time": "2023-05-12T05:52:30.161782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11324 7098\n"
     ]
    }
   ],
   "source": [
    "# models\n",
    "LR = LogisticRegression(random_state=RANDOM_STATE)\n",
    "RF = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "DTC = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "KNC = KNeighborsClassifier()\n",
    "SVC = svm.SVC(random_state=RANDOM_STATE)\n",
    "GNB = GaussianNB()\n",
    "LGBM = LGBMRanker(objective=\"lambdarank\",random_state=RANDOM_STATE)\n",
    "\n",
    "train_features=['density', 'entropy', 'competitiveness','area_pop']\n",
    "# train_features=['density', 'entropy', 'competitiveness','area_pop', 'accessibility','complementary','affinity']\n",
    "# train_features=['density', 'entropy', 'competitiveness','area_pop', 'accessibility','cosine_sim']\n",
    "# train_features=['density', 'entropy', 'competitiveness','area_pop', 'accessibility','complementary_with_att','affinity_with_att']\n",
    "\n",
    "\n",
    "get_group_size = lambda df: df.reset_index().groupby(\"business_id\")['business_id'].count()\n",
    "\n",
    "train_groups = get_group_size(train_df).to_numpy()\n",
    "test_groups = get_group_size(test_df).to_numpy()\n",
    "\n",
    "print(sum(train_groups) , sum(test_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "future-oxford",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:52:30.636910Z",
     "start_time": "2023-05-12T05:52:30.320506Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/emma/bilab/Steph_C/steph_env/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7dee9e765f51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'LGBMRanker'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relevance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relevance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/emma/bilab/Steph_C/steph_env/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1344\u001b[0m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[1;32m   1345\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m                                    accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1347\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/emma/bilab/Steph_C/steph_env/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/emma/bilab/Steph_C/steph_env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/emma/bilab/Steph_C/steph_env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/emma/bilab/Steph_C/steph_env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n\u001b[0;32m--> 106\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m    107\u001b[0m             )\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "models = [LR, RF, DTC, KNC, SVC, GNB, LGBM]\n",
    "model_name =['LR', 'RF', 'DTC', 'KNC', 'SVC', 'GNB','LGBMRanker']\n",
    "score_dict = {}\n",
    "\n",
    "for i in range(len(models)):\n",
    "    score_dict[model_name[i]]={}\n",
    "    model = models[i]\n",
    "    # Train\n",
    "    if model_name[i] != 'LGBMRanker':\n",
    "        model.fit(train_df[train_features], train_df[['relevance']])\n",
    "    else:\n",
    "        model.fit(train_df[train_features], train_df[['relevance']], group=train_groups)\n",
    "    \n",
    "    # Predict\n",
    "    predict = model.predict(test_df[train_features])\n",
    "    test_df['predictions'] = predict\n",
    "    pred_list, true_rel , true_list , output_dict = get_ranking(test_df)\n",
    "\n",
    "    # Evaluation\n",
    "    mrr_list = calculate_mrr(pred_list , true_list)\n",
    "    map_list = calculate_map(pred_list , true_list)\n",
    "    dcg_list , ndcg_list = calculate_dcg_ndcg(pred_list , true_list,true_rel)\n",
    "    precision_list_1 = _precision(pred_list , true_list, k=1)\n",
    "    recall_list_1 = _recall(pred_list , true_list,k=1)\n",
    "    precision_list_3 = _precision(pred_list , true_list, k=3)\n",
    "    recall_list_3 = _recall(pred_list , true_list,k=3)\n",
    "    precision_list = _precision(pred_list , true_list)\n",
    "    recall_list = _recall(pred_list , true_list)\n",
    "\n",
    "    score_dict[model_name[i]]['precision @ 1'] = np.mean(precision_list_1)\n",
    "#     score_dict[model_name[i]]['recall @ 1'] = np.mean(recall_list_1)\n",
    "    score_dict[model_name[i]]['precision @ 3 '] = np.mean(precision_list_3)\n",
    "    score_dict[model_name[i]]['recall @ 3'] = np.mean(recall_list_3)\n",
    "    score_dict[model_name[i]]['precision'] = np.mean(precision_list)\n",
    "    score_dict[model_name[i]]['recall'] = np.mean(recall_list)\n",
    "    score_dict[model_name[i]]['mrr'] = np.mean(mrr_list)\n",
    "    score_dict[model_name[i]]['map'] = np.mean(map_list)\n",
    "    score_dict[model_name[i]]['dcg'] = np.mean(dcg_list)\n",
    "    score_dict[model_name[i]]['ndcg'] = np.mean(ndcg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sporting-norwegian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:58:14.264430Z",
     "start_time": "2023-05-12T05:58:14.142240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11324 entries, 0 to 11323\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   business_id      11324 non-null  object \n",
      " 1   name             11324 non-null  object \n",
      " 2   postal_code      11324 non-null  object \n",
      " 3   review_count     11324 non-null  object \n",
      " 4   categories       11324 non-null  object \n",
      " 5   density          11324 non-null  float64\n",
      " 6   entropy          11324 non-null  float64\n",
      " 7   area_pop         11324 non-null  float64\n",
      " 8   competitiveness  11324 non-null  float64\n",
      " 9   relevance        10188 non-null  float64\n",
      "dtypes: float64(5), object(5)\n",
      "memory usage: 884.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-ladder",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-lingerie",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:52:30.645143Z",
     "start_time": "2023-05-12T05:52:25.050Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(score_dict).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-meaning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:52:30.646367Z",
     "start_time": "2023-05-12T05:52:25.052Z"
    }
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(output_dict).T.to_csv('./output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-beauty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T05:52:30.647574Z",
     "start_time": "2023-05-12T05:52:25.054Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(output_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-paste",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steph_env",
   "language": "python",
   "name": "steph_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
