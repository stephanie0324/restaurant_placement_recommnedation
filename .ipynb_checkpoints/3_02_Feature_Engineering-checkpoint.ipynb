{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "floating-compression",
   "metadata": {},
   "source": [
    "# Create the features DF\n",
    "* using by_postal_code dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-perspective",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-trance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T05:17:43.478183Z",
     "start_time": "2023-05-14T05:17:42.347678Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import math \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-favor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T05:17:43.507094Z",
     "start_time": "2023-05-14T05:17:43.505070Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-camel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T05:17:44.819028Z",
     "start_time": "2023-05-14T05:17:43.532158Z"
    }
   },
   "outputs": [],
   "source": [
    "# 導入資料\n",
    "train_df = pd.read_pickle('../data/Train_by_postoal_code_without_review_pointwise_v3_3.pkl').reset_index(drop=True)\n",
    "test_df = pd.read_pickle('../data/Test_by_postoal_code_without_review_pointwise_v3_3.pkl').reset_index(drop=True)\n",
    "postal_code_feature_dict = pickle.load(open(\"../data/postal_dict.pkl\", \"rb\"))\n",
    "docs = pickle.load(open(\"../data/LDA_training_docs.pkl\", \"rb\"))\n",
    "all_df = pd.read_pickle('../Data/restaurant_only.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-emergency",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T05:17:48.991224Z",
     "start_time": "2023-05-14T05:17:48.988012Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train_df.shape, test_df.shape, all_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-bargain",
   "metadata": {},
   "source": [
    "# LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-satisfaction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T05:18:19.964671Z",
     "start_time": "2023-05-14T05:18:18.893546Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import matutils\n",
    "#python -m spacy download en_core_web_md \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-creature",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T05:18:25.970708Z",
     "start_time": "2023-05-14T05:18:24.156112Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "keep_pos = ['NOUN','ADJ','ADV','VERB']\n",
    "# removal= ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n",
    "\n",
    "def preprocess(text : str) -> list:\n",
    "    \n",
    "    for summary in nlp.pipe([text]):\n",
    "        proj_tok = [token.lemma_.lower() for token in summary \\\n",
    "                    if token.pos_ in keep_pos and not token.is_stop and token.is_alpha]\n",
    "\n",
    "    return proj_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-interpretation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T05:18:28.671580Z",
     "start_time": "2023-05-14T05:18:28.669381Z"
    }
   },
   "outputs": [],
   "source": [
    "# # unhash if necessary \n",
    "# def get_training_docs(all_df):\n",
    "    \n",
    "#     docs = []\n",
    "    \n",
    "#     for name in Counter(all_df.name):\n",
    "#         res = all_df[all_df.name == name]\n",
    "        \n",
    "#         docs.append(preprocess(''.join(res.text.values.tolist())[:5000]))\n",
    "\n",
    "#     return docs\n",
    "\n",
    "# docs = get_training_docs(all_df)\n",
    "# with open(\"../data/LDA_training_docs.pkl\",\"wb\") as file:\n",
    "#     # Use pickle.dump() to serialize and save the object to the file\n",
    "#     pickle.dump(docs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-fusion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T05:18:41.887476Z",
     "start_time": "2023-05-14T05:18:32.846912Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=50, no_above=0.5,keep_n=1000)\n",
    "\n",
    "# Train LDA\n",
    "# #### doc2Bow\n",
    "# corpus = [dictionary.doc2bow(doc) for doc in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-danish",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T05:18:46.068034Z",
     "start_time": "2023-05-14T05:18:46.066128Z"
    }
   },
   "outputs": [],
   "source": [
    "# print('Number of unique tokens: %d' % len(dictionary))\n",
    "# print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-alarm",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T05:18:50.231748Z",
     "start_time": "2023-05-14T05:18:50.229889Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Train LDA\n",
    "# # # building models\n",
    "# lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=50, \\\n",
    "#                          num_topics=50, workers = 4, passes=10)\n",
    "\n",
    "# with open('../data/lda_model.pkl', 'wb') as file:\n",
    "#     # Use pickle.dump() to serialize and save the object to the file\n",
    "#     pickle.dump(lda_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-story",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T05:18:54.415296Z",
     "start_time": "2023-05-14T05:18:54.397847Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_model = pickle.load(open('../data/lda_model.pkl', 'rb'))\n",
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-immune",
   "metadata": {},
   "source": [
    "# Review Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-yemen",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T05:18:58.780480Z",
     "start_time": "2023-05-14T05:18:58.598927Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "# Download the General Inquirer lexicon\n",
    "nltk.download('opinion_lexicon')\n",
    "general_inquirer_words = set(opinion_lexicon.words())\n",
    "\n",
    "# Function to check if a string contains General Inquirer words\n",
    "def contains_general_inquirer(text):\n",
    "    words = set(text.lower().split())\n",
    "    common_words = words.intersection(general_inquirer_words)\n",
    "    return len(common_words) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-joining",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T05:19:02.959544Z",
     "start_time": "2023-05-14T05:19:02.957481Z"
    }
   },
   "outputs": [],
   "source": [
    "# # unhash if necessary\n",
    "# cnt = 0\n",
    "# for idx , row in all_df.iterrows():\n",
    "#     # if cnt%10000 == 0:\n",
    "#     #     print(f'Now progress .... {cnt}')\n",
    "#     if contains_general_inquirer(row.text):\n",
    "#         pass\n",
    "#     else:\n",
    "#         cnt+=1\n",
    "#         row.text = ''\n",
    "# all_df.to_pickle('../Data/restaurant_only_filtered.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-baptist",
   "metadata": {},
   "source": [
    "## Get Review Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-yesterday",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T05:19:07.145170Z",
     "start_time": "2023-05-14T05:19:07.136738Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_LDA_aspects(df , all_df):\n",
    "\n",
    "    df['LDA_res'] = ''\n",
    "    df['LDA_loc'] = ''\n",
    "\n",
    "    for idx , row in df.iterrows():\n",
    "        if idx % 1000 ==0 :\n",
    "            print(f'Now progress... {idx}')\n",
    "        res = all_df[all_df.business_id == row.business_id].sort_values(by='date',ascending=False)\n",
    "        loc = all_df[all_df.postal_code == row.postal_code].sort_values(by='date',ascending=False)\n",
    "        \n",
    "        try:\n",
    "            res_str = ''.join(list(res.text))[:3000]\n",
    "            loc_str = ''.join(list(res.text))[:3000]\n",
    "\n",
    "            corpus_res = [dictionary.doc2bow(doc) for doc in [preprocess(res_str)]]\n",
    "            corpus_loc = [dictionary.doc2bow(doc) for doc in [preprocess(loc_str)]]\n",
    "\n",
    "            df['LDA_res'][idx] = lda_model[corpus_res][0]\n",
    "            df['LDA_loc'][idx] = lda_model[corpus_loc][0]\n",
    "            \n",
    "        except:\n",
    "            df['LDA_res'][idx] = []\n",
    "            df['LDA_loc'][idx] = []\n",
    "        \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-porter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T06:44:28.831749Z",
     "start_time": "2023-05-14T05:19:11.338062Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = get_LDA_aspects(train_df , all_df)\n",
    "test_df = get_LDA_aspects(test_df , all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-contest",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T06:44:33.790564Z",
     "start_time": "2023-05-14T06:44:33.171300Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.to_pickle('../data/Train_by_postoal_code_pointwise_v3_3.pkl')\n",
    "test_df.to_pickle('../data/Test_by_postoal_code_pointwise_v3_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-louisiana",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T07:04:44.128751Z",
     "start_time": "2023-05-14T07:04:43.979560Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('../data/Train_by_postoal_code_pointwise_v3_3.pkl')\n",
    "test_df = pd.read_pickle('../data/Test_by_postoal_code_pointwise_v3_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-people",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T07:04:54.819658Z",
     "start_time": "2023-05-14T07:04:50.827259Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased',output_hidden_states = True)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def get_bert_embedding(df , all_df):\n",
    "    \n",
    "    df['res_emb'] = ''\n",
    "    df['loc_emb'] = ''\n",
    "    \n",
    "    for idx , row in df.iterrows():\n",
    "        if idx % 1000 ==0 :\n",
    "            print(f'Now progress... {idx}')\n",
    "        res = all_df[all_df.business_id == row.business_id].sort_values(by='date',ascending=False)\n",
    "        loc = all_df[all_df.postal_code == row.postal_code].sort_values(by='date',ascending=False)\n",
    "        \n",
    "        res_embedding = []\n",
    "        loc_embedding = []\n",
    "        \n",
    "        try:\n",
    "\n",
    "            for _idx , _row in res.iterrows():\n",
    "                encoded_input = tokenizer(_row.text, max_length = 128 , padding = True , truncation = True,  return_tensors='pt')\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**encoded_input)\n",
    "                res_embedding.append(outputs.hidden_states[-1][0,0,:])\n",
    "            df['res_emb'][idx] = torch.mean(torch.stack(res_embedding), dim=0)\n",
    "        except:\n",
    "            df['res_emb'][idx] = torch.zeros([1, 768], dtype=torch.int32)\n",
    "\n",
    "        try:\n",
    "            for _idx , _row in loc.iterrows():\n",
    "                encoded_input = tokenizer(_row.text, max_length = 128 , padding = True , truncation = True,  return_tensors='pt')\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**encoded_input)\n",
    "                loc_embedding.append(outputs.hidden_states[-1][0,0,:])\n",
    "            df['loc_emb'][idx] =torch.mean(torch.stack(loc_embedding), dim=0)\n",
    "        except:\n",
    "            df['loc_emb'][idx] = torch.zeros([1, 768], dtype=torch.int32)\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-columbia",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-14T07:04:54.664Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = get_bert_embedding(train_df , all_df)\n",
    "test_df = get_bert_embedding(test_df , all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-metropolitan",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-14T07:05:00.629Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.shape , test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-sight",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-14T07:05:09.448Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.to_pickle('../data/Train_by_postoal_code_pointwise_v3_3.pkl')\n",
    "test_df.to_pickle('../data/Test_by_postoal_code_pointwise_v3_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-victorian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s_env",
   "language": "python",
   "name": "s_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f457e4bdb0a834eef26de468bbb8aa330d736c8b7db558705075494c4e15f1a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
