{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nervous-rocket",
   "metadata": {},
   "source": [
    "# Create Apects and Embedding for every Review\n",
    "* using all_df_filtered dataset\n",
    "* get the topics(aspect) from reviews (LDA)\n",
    "* get BERT embedding from reviews\n",
    "* get aspect score from reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-booking",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "irish-straight",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:11:41.876778Z",
     "start_time": "2023-05-17T05:11:41.644007Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "outer-groove",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:11:42.632645Z",
     "start_time": "2023-05-17T05:11:41.984930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "marine-hands",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:11:42.652475Z",
     "start_time": "2023-05-17T05:11:42.650453Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hearing-italian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:11:43.077175Z",
     "start_time": "2023-05-17T05:11:42.678144Z"
    }
   },
   "outputs": [],
   "source": [
    "# 導入資料\n",
    "all_df = pd.read_pickle('../Data/restaurant_only_s_with_embedding.pkl').reset_index(drop=True)\n",
    "lda_model = pickle.load(open('../data/lda_model_v2.pkl', 'rb'))\n",
    "docs = pickle.load(open(\"../data/LDA_training_docs_v2.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "valid-index",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:11:44.955783Z",
     "start_time": "2023-05-17T05:11:44.952935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24852, 24)\n"
     ]
    }
   ],
   "source": [
    "print(all_df.shape)\n",
    "# print(f'Number of unique tokens: {len(dictionary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-harvard",
   "metadata": {},
   "source": [
    "# Review Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-multiple",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## General Inquirer\n",
    "* Most reviews have general inquirer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-sullivan",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import opinion_lexicon\n",
    "\n",
    "# # Download the General Inquirer lexicon\n",
    "# nltk.download('opinion_lexicon')\n",
    "# general_inquirer_words = set(opinion_lexicon.words())\n",
    "\n",
    "# # Function to check if a string contains General Inquirer words\n",
    "# def contains_general_inquirer(text):\n",
    "#     words = set(text.lower().split())\n",
    "#     common_words = words.intersection(general_inquirer_words)\n",
    "#     return len(common_words) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-alliance",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # unhash if necessary\n",
    "# cnt = 0\n",
    "# for idx , row in all_df.iterrows():\n",
    "#     # if cnt%10000 == 0:\n",
    "#     #     print(f'Now progress .... {cnt}')\n",
    "#     if contains_general_inquirer(row.text):\n",
    "#         pass\n",
    "#     else:\n",
    "#         cnt+=1\n",
    "#         row.text = ''\n",
    "# all_df.to_pickle('../Data/restaurant_only_filtered.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-brave",
   "metadata": {},
   "source": [
    "## LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "foreign-celtic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:11:51.351919Z",
     "start_time": "2023-05-17T05:11:50.436806Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import matutils\n",
    "#python -m spacy download en_core_web_md \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "final-rochester",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:11:54.228531Z",
     "start_time": "2023-05-17T05:11:51.502734Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "keep_pos = ['NOUN','ADJ','ADV','VERB']\n",
    "# removal= ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n",
    "\n",
    "def preprocess(text : str) -> list:\n",
    "    \n",
    "    for summary in nlp.pipe([text]):\n",
    "        proj_tok = [token.lemma_.lower() for token in summary \\\n",
    "                    if token.pos_ in keep_pos and not token.is_stop and token.is_alpha]\n",
    "\n",
    "    return proj_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-psychiatry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T15:46:19.361631Z",
     "start_time": "2023-05-16T15:45:07.570883Z"
    }
   },
   "outputs": [],
   "source": [
    "# # unhash if necessary \n",
    "# def get_training_docs(all_df):\n",
    "    \n",
    "#     docs = []\n",
    "    \n",
    "#     for name in Counter(all_df.name):\n",
    "#         res = all_df[all_df.name == name]\n",
    "        \n",
    "#         docs.append(preprocess(''.join(res.text.values.tolist())[:5000]))\n",
    "\n",
    "#     return docs\n",
    "\n",
    "# docs = get_training_docs(all_df)\n",
    "# with open(\"../data/LDA_training_docs_v2.pkl\",\"wb\") as file:\n",
    "#     # Use pickle.dump() to serialize and save the object to the file\n",
    "#     pickle.dump(docs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-plain",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T15:47:50.770008Z",
     "start_time": "2023-05-16T15:47:38.105839Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Train LDA\n",
    "\n",
    "# # Create a dictionary representation of the documents.\n",
    "# dictionary = Dictionary(docs)\n",
    "\n",
    "# # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "# dictionary.filter_extremes(no_below=50, no_above=0.5,keep_n=1000)\n",
    "\n",
    "# #### doc2Bow\n",
    "# corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# # # building models\n",
    "# lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, iterations=50, \\\n",
    "#                          num_topics=50, workers = 4, passes=10)\n",
    "\n",
    "# with open('../data/lda_model_v2.pkl', 'wb') as file:\n",
    "#     # Use pickle.dump() to serialize and save the object to the file\n",
    "#     pickle.dump(lda_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "biblical-collector",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:11:54.681424Z",
     "start_time": "2023-05-17T05:11:54.363378Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=50, no_above=0.5,keep_n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "confidential-strand",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:11:54.824191Z",
     "start_time": "2023-05-17T05:11:54.814742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.060*\"sandwich\" + 0.025*\"quick\" + 0.019*\"meat\" + 0.018*\"tasty\" + 0.017*\"wing\" + 0.016*\"pick\" + 0.014*\"guy\" + 0.013*\"delivery\" + 0.013*\"beef\" + 0.012*\"different\"'),\n",
       " (1,\n",
       "  '0.029*\"fry\" + 0.029*\"table\" + 0.029*\"potato\" + 0.029*\"lady\" + 0.029*\"drive\" + 0.021*\"tip\" + 0.021*\"friend\" + 0.021*\"min\" + 0.021*\"sit\" + 0.021*\"bring\"'),\n",
       " (2,\n",
       "  '0.077*\"breakfast\" + 0.030*\"coffee\" + 0.027*\"egg\" + 0.026*\"toast\" + 0.022*\"server\" + 0.020*\"table\" + 0.013*\"bacon\" + 0.012*\"feel\" + 0.012*\"option\" + 0.011*\"sit\"'),\n",
       " (3,\n",
       "  '0.059*\"pork\" + 0.037*\"bean\" + 0.029*\"sweet\" + 0.020*\"meat\" + 0.018*\"cold\" + 0.017*\"table\" + 0.017*\"drive\" + 0.017*\"rice\" + 0.017*\"sit\" + 0.016*\"home\"'),\n",
       " (4,\n",
       "  '0.043*\"rice\" + 0.031*\"onion\" + 0.022*\"pepper\" + 0.022*\"veggie\" + 0.021*\"quick\" + 0.020*\"egg\" + 0.019*\"lot\" + 0.019*\"fast\" + 0.019*\"roll\" + 0.016*\"base\"'),\n",
       " (5,\n",
       "  '0.020*\"pay\" + 0.017*\"employee\" + 0.016*\"manager\" + 0.015*\"shrimp\" + 0.014*\"review\" + 0.013*\"area\" + 0.013*\"charge\" + 0.012*\"spicy\" + 0.012*\"night\" + 0.012*\"wrong\"'),\n",
       " (6,\n",
       "  '0.032*\"rice\" + 0.021*\"roll\" + 0.017*\"dinner\" + 0.015*\"fry\" + 0.015*\"review\" + 0.014*\"piece\" + 0.013*\"item\" + 0.013*\"close\" + 0.013*\"quality\" + 0.011*\"plate\"'),\n",
       " (7,\n",
       "  '0.101*\"wing\" + 0.030*\"steak\" + 0.028*\"wrap\" + 0.027*\"night\" + 0.027*\"combo\" + 0.026*\"late\" + 0.026*\"drive\" + 0.024*\"town\" + 0.021*\"picture\" + 0.020*\"delivery\"'),\n",
       " (8,\n",
       "  '0.058*\"pizza\" + 0.023*\"kid\" + 0.017*\"mask\" + 0.015*\"home\" + 0.014*\"large\" + 0.014*\"min\" + 0.013*\"ready\" + 0.013*\"call\" + 0.013*\"box\" + 0.012*\"bread\"'),\n",
       " (9,\n",
       "  '0.129*\"sandwich\" + 0.040*\"coffee\" + 0.017*\"bread\" + 0.014*\"lunch\" + 0.013*\"shop\" + 0.013*\"lot\" + 0.013*\"tomato\" + 0.012*\"spot\" + 0.012*\"visit\" + 0.011*\"option\"'),\n",
       " (10,\n",
       "  '0.025*\"call\" + 0.024*\"phone\" + 0.023*\"busy\" + 0.022*\"answer\" + 0.020*\"waiter\" + 0.020*\"maybe\" + 0.019*\"family\" + 0.019*\"arrive\" + 0.017*\"dinner\" + 0.016*\"able\"'),\n",
       " (11,\n",
       "  '0.032*\"rice\" + 0.027*\"pork\" + 0.020*\"soup\" + 0.020*\"fry\" + 0.017*\"dish\" + 0.017*\"beef\" + 0.016*\"shrimp\" + 0.016*\"review\" + 0.014*\"feel\" + 0.013*\"extra\"'),\n",
       " (12,\n",
       "  '0.032*\"pizza\" + 0.022*\"kid\" + 0.018*\"sit\" + 0.017*\"manager\" + 0.017*\"salad\" + 0.017*\"bar\" + 0.016*\"super\" + 0.016*\"receive\" + 0.016*\"wing\" + 0.015*\"show\"'),\n",
       " (13,\n",
       "  '0.157*\"taco\" + 0.030*\"drink\" + 0.026*\"beef\" + 0.022*\"pick\" + 0.018*\"add\" + 0.017*\"use\" + 0.016*\"spot\" + 0.015*\"different\" + 0.014*\"worth\" + 0.014*\"ago\"'),\n",
       " (14,\n",
       "  '0.004*\"taco\" + 0.003*\"drink\" + 0.003*\"wife\" + 0.003*\"fry\" + 0.003*\"start\" + 0.003*\"lot\" + 0.003*\"table\" + 0.003*\"bean\" + 0.003*\"server\" + 0.003*\"bite\"'),\n",
       " (15,\n",
       "  '0.057*\"flavor\" + 0.048*\"bread\" + 0.047*\"visit\" + 0.038*\"friend\" + 0.035*\"outside\" + 0.032*\"extra\" + 0.029*\"light\" + 0.026*\"portion\" + 0.026*\"dining\" + 0.026*\"week\"'),\n",
       " (16,\n",
       "  '0.050*\"flavor\" + 0.041*\"takeout\" + 0.041*\"dish\" + 0.025*\"spot\" + 0.025*\"helpful\" + 0.025*\"option\" + 0.025*\"bit\" + 0.025*\"feel\" + 0.017*\"fantastic\" + 0.017*\"phone\"'),\n",
       " (17,\n",
       "  '0.037*\"salad\" + 0.023*\"lunch\" + 0.021*\"clean\" + 0.018*\"wrong\" + 0.018*\"bread\" + 0.018*\"drive\" + 0.016*\"meat\" + 0.015*\"sandwich\" + 0.015*\"home\" + 0.014*\"call\"'),\n",
       " (18,\n",
       "  '0.042*\"burger\" + 0.039*\"drink\" + 0.034*\"fry\" + 0.031*\"wing\" + 0.023*\"server\" + 0.018*\"beer\" + 0.012*\"cook\" + 0.012*\"visit\" + 0.012*\"bar\" + 0.011*\"big\"'),\n",
       " (19,\n",
       "  '0.100*\"taco\" + 0.031*\"chip\" + 0.016*\"shrimp\" + 0.015*\"bean\" + 0.013*\"meat\" + 0.013*\"flavor\" + 0.013*\"pork\" + 0.013*\"beef\" + 0.012*\"rice\" + 0.010*\"dry\"'),\n",
       " (20,\n",
       "  '0.042*\"bowl\" + 0.033*\"rice\" + 0.027*\"spicy\" + 0.026*\"roll\" + 0.022*\"portion\" + 0.020*\"egg\" + 0.020*\"beef\" + 0.020*\"flavor\" + 0.020*\"pork\" + 0.014*\"large\"'),\n",
       " (21,\n",
       "  '0.030*\"server\" + 0.023*\"fry\" + 0.019*\"table\" + 0.017*\"friend\" + 0.017*\"stop\" + 0.017*\"open\" + 0.016*\"feel\" + 0.014*\"dinner\" + 0.013*\"today\" + 0.013*\"run\"'),\n",
       " (22,\n",
       "  '0.117*\"roll\" + 0.028*\"shrimp\" + 0.022*\"spicy\" + 0.021*\"area\" + 0.020*\"decent\" + 0.019*\"tasty\" + 0.018*\"call\" + 0.014*\"item\" + 0.013*\"yummy\" + 0.012*\"new\"'),\n",
       " (23,\n",
       "  '0.081*\"wing\" + 0.047*\"fry\" + 0.047*\"tender\" + 0.030*\"meat\" + 0.030*\"dinner\" + 0.030*\"fish\" + 0.028*\"garlic\" + 0.023*\"flavor\" + 0.019*\"quality\" + 0.016*\"share\"'),\n",
       " (24,\n",
       "  '0.040*\"bar\" + 0.040*\"drink\" + 0.037*\"table\" + 0.037*\"waitress\" + 0.032*\"server\" + 0.028*\"beer\" + 0.027*\"manager\" + 0.025*\"sit\" + 0.021*\"seat\" + 0.019*\"walk\"'),\n",
       " (25,\n",
       "  '0.036*\"table\" + 0.028*\"mask\" + 0.020*\"close\" + 0.019*\"flavor\" + 0.017*\"wear\" + 0.016*\"serve\" + 0.016*\"kid\" + 0.016*\"fast\" + 0.014*\"clean\" + 0.014*\"party\"'),\n",
       " (26,\n",
       "  '0.005*\"manager\" + 0.004*\"salad\" + 0.004*\"bar\" + 0.004*\"waitress\" + 0.004*\"start\" + 0.003*\"lunch\" + 0.003*\"roll\" + 0.003*\"fry\" + 0.003*\"end\" + 0.003*\"server\"'),\n",
       " (27,\n",
       "  '0.046*\"breakfast\" + 0.034*\"server\" + 0.025*\"egg\" + 0.021*\"fry\" + 0.021*\"coffee\" + 0.016*\"steak\" + 0.013*\"table\" + 0.013*\"family\" + 0.013*\"close\" + 0.013*\"portion\"'),\n",
       " (28,\n",
       "  '0.058*\"shrimp\" + 0.039*\"steak\" + 0.030*\"fry\" + 0.021*\"cook\" + 0.019*\"fried\" + 0.014*\"potato\" + 0.014*\"meat\" + 0.014*\"fish\" + 0.013*\"clean\" + 0.013*\"feel\"'),\n",
       " (29,\n",
       "  '0.023*\"table\" + 0.020*\"steak\" + 0.020*\"server\" + 0.015*\"dinner\" + 0.013*\"drink\" + 0.012*\"portion\" + 0.012*\"family\" + 0.011*\"serve\" + 0.011*\"salad\" + 0.010*\"sit\"'),\n",
       " (30,\n",
       "  '0.036*\"chip\" + 0.033*\"flavor\" + 0.026*\"bar\" + 0.018*\"clean\" + 0.015*\"drink\" + 0.014*\"slow\" + 0.014*\"stop\" + 0.014*\"atmosphere\" + 0.013*\"review\" + 0.012*\"issue\"'),\n",
       " (31,\n",
       "  '0.028*\"fry\" + 0.024*\"rice\" + 0.018*\"pork\" + 0.017*\"flavor\" + 0.014*\"beef\" + 0.014*\"egg\" + 0.014*\"fried\" + 0.013*\"fast\" + 0.012*\"quick\" + 0.012*\"roll\"'),\n",
       " (32,\n",
       "  '0.233*\"sandwich\" + 0.081*\"grill\" + 0.038*\"money\" + 0.033*\"bread\" + 0.031*\"spend\" + 0.030*\"pandemic\" + 0.029*\"meat\" + 0.025*\"bag\" + 0.023*\"care\" + 0.022*\"old\"'),\n",
       " (33,\n",
       "  '0.030*\"dish\" + 0.019*\"portion\" + 0.019*\"small\" + 0.018*\"quality\" + 0.016*\"takeout\" + 0.013*\"excellent\" + 0.013*\"area\" + 0.013*\"spicy\" + 0.012*\"bring\" + 0.011*\"shrimp\"'),\n",
       " (34,\n",
       "  '0.058*\"fry\" + 0.024*\"ice\" + 0.022*\"cream\" + 0.018*\"sandwich\" + 0.013*\"fast\" + 0.011*\"close\" + 0.009*\"treat\" + 0.009*\"review\" + 0.009*\"stop\" + 0.009*\"warm\"'),\n",
       " (35,\n",
       "  '0.166*\"pizza\" + 0.024*\"crust\" + 0.016*\"salad\" + 0.012*\"delivery\" + 0.010*\"topping\" + 0.010*\"thin\" + 0.009*\"pick\" + 0.009*\"large\" + 0.008*\"wing\" + 0.007*\"cold\"'),\n",
       " (36,\n",
       "  '0.028*\"store\" + 0.025*\"drink\" + 0.023*\"coffee\" + 0.019*\"parking\" + 0.018*\"clean\" + 0.016*\"sweet\" + 0.016*\"visit\" + 0.012*\"sit\" + 0.012*\"area\" + 0.012*\"stop\"'),\n",
       " (37,\n",
       "  '0.004*\"pizza\" + 0.003*\"owner\" + 0.003*\"soup\" + 0.003*\"fry\" + 0.003*\"delivery\" + 0.003*\"check\" + 0.003*\"drink\" + 0.003*\"pay\" + 0.003*\"walk\" + 0.003*\"table\"'),\n",
       " (38,\n",
       "  '0.132*\"burger\" + 0.055*\"fry\" + 0.017*\"onion\" + 0.014*\"fast\" + 0.013*\"quality\" + 0.013*\"cook\" + 0.012*\"new\" + 0.010*\"feel\" + 0.010*\"drive\" + 0.009*\"car\"'),\n",
       " (39,\n",
       "  '0.033*\"soup\" + 0.020*\"salad\" + 0.016*\"potato\" + 0.014*\"pay\" + 0.013*\"lot\" + 0.013*\"bring\" + 0.013*\"option\" + 0.012*\"stop\" + 0.012*\"review\" + 0.012*\"shrimp\"'),\n",
       " (40,\n",
       "  '0.034*\"meat\" + 0.025*\"open\" + 0.020*\"lunch\" + 0.019*\"hand\" + 0.019*\"bring\" + 0.018*\"girl\" + 0.016*\"guy\" + 0.015*\"waiter\" + 0.014*\"clean\" + 0.014*\"disappoint\"'),\n",
       " (41,\n",
       "  '0.029*\"server\" + 0.022*\"drink\" + 0.022*\"seat\" + 0.021*\"table\" + 0.020*\"manager\" + 0.018*\"hour\" + 0.014*\"check\" + 0.014*\"waitress\" + 0.014*\"night\" + 0.013*\"option\"'),\n",
       " (42,\n",
       "  '0.048*\"coffee\" + 0.024*\"drink\" + 0.023*\"bar\" + 0.023*\"visit\" + 0.023*\"close\" + 0.022*\"cold\" + 0.021*\"box\" + 0.021*\"husband\" + 0.019*\"area\" + 0.018*\"fantastic\"'),\n",
       " (43,\n",
       "  '0.078*\"breakfast\" + 0.056*\"egg\" + 0.041*\"bacon\" + 0.019*\"pizza\" + 0.015*\"cook\" + 0.015*\"small\" + 0.014*\"hard\" + 0.013*\"different\" + 0.013*\"onion\" + 0.013*\"big\"'),\n",
       " (44,\n",
       "  '0.159*\"bowl\" + 0.022*\"option\" + 0.016*\"line\" + 0.014*\"add\" + 0.014*\"visit\" + 0.013*\"choice\" + 0.011*\"quick\" + 0.011*\"rice\" + 0.010*\"huge\" + 0.010*\"person\"'),\n",
       " (45,\n",
       "  '0.055*\"pay\" + 0.035*\"bar\" + 0.028*\"management\" + 0.028*\"state\" + 0.021*\"table\" + 0.021*\"let\" + 0.021*\"cold\" + 0.021*\"manager\" + 0.021*\"regular\" + 0.021*\"decide\"'),\n",
       " (46,\n",
       "  '0.027*\"drive\" + 0.026*\"employee\" + 0.020*\"manager\" + 0.018*\"pay\" + 0.014*\"money\" + 0.014*\"phone\" + 0.014*\"walk\" + 0.014*\"worker\" + 0.013*\"call\" + 0.012*\"hour\"'),\n",
       " (47,\n",
       "  '0.040*\"lunch\" + 0.029*\"bar\" + 0.029*\"salad\" + 0.029*\"fish\" + 0.023*\"kid\" + 0.020*\"half\" + 0.020*\"start\" + 0.017*\"help\" + 0.017*\"expect\" + 0.017*\"fried\"'),\n",
       " (48,\n",
       "  '0.005*\"pizza\" + 0.004*\"burger\" + 0.004*\"crust\" + 0.004*\"excellent\" + 0.003*\"meat\" + 0.003*\"flavor\" + 0.003*\"waitress\" + 0.003*\"beef\" + 0.003*\"thin\" + 0.003*\"lunch\"'),\n",
       " (49,\n",
       "  '0.116*\"salad\" + 0.017*\"lunch\" + 0.016*\"employee\" + 0.016*\"soup\" + 0.015*\"pick\" + 0.014*\"bar\" + 0.014*\"bread\" + 0.013*\"sandwich\" + 0.011*\"lettuce\" + 0.011*\"option\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check topic\n",
    "lda_model.print_topics(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-clarity",
   "metadata": {},
   "source": [
    "## Bert Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fatal-collection",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:12:07.881706Z",
     "start_time": "2023-05-17T05:11:56.743714Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name,output_hidden_states = True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-switzerland",
   "metadata": {},
   "source": [
    "## Aspect Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "laughing-system",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:14:02.842357Z",
     "start_time": "2023-05-17T05:14:02.839411Z"
    }
   },
   "outputs": [],
   "source": [
    "# given aspects\n",
    "restaurant_aspects = [\n",
    "    \"Food Quality\",\n",
    "    \"Technique\",\n",
    "    \"Ingredients\",\n",
    "    \"Creativity\",\n",
    "    \"Consistency\",\n",
    "    \"Value for Money\",\n",
    "    \"Mastery of Flavors\",\n",
    "    \"Presentation\",\n",
    "    \"Dining Experience\",\n",
    "    \"Ambiance\",\n",
    "    \"Wine and Beverage Selection\",\n",
    "    \"Service\",\n",
    "    \"Attention to Detail\",\n",
    "    \"Chef's Vision\",\n",
    "    \"Authenticity\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "agreed-religion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:14:02.688098Z",
     "start_time": "2023-05-17T05:14:02.681716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the most important words for each topic and turn it into aspects \n",
    "num_words = 10  # Number of top words to retrieve for each topic\n",
    "topics = lda_model.show_topics(num_topics=-1, num_words=num_words, formatted=False)\n",
    "LDA_aspect_list = []\n",
    "# Print the most important words for each topic\n",
    "for topic_id, topic in topics:\n",
    "    # print(f'Topic #{topic_id} , {topic[0][0]}')\n",
    "    for word, _ in topic:\n",
    "        if word not in LDA_aspect_list:\n",
    "            LDA_aspect_list.append(word)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "great-rogers",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:14:02.533211Z",
     "start_time": "2023-05-17T05:13:54.321884Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,DebertaV2Tokenizer\n",
    "\n",
    "absa_tokenizer = AutoTokenizer.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\")\n",
    "absa_model = AutoModelForSequenceClassification.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\").to(device)\n",
    "# inputs = tokenizer(\"[CLS] when tables opened up, the manager sat another party before us. [SEP] manager [SEP]\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)\n",
    "# SequenceClassifierOutput(loss=None, logits=tensor([[ 4.3158, -2.3969, -1.7158]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-figure",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "undefined-button",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T09:25:22.594577Z",
     "start_time": "2023-05-17T09:25:22.587126Z"
    }
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "def get_aspect(text):\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in [preprocess(text)]]\n",
    "    return lda_model[corpus][0]\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    encoded_input = bert_tokenizer(text, max_length = 128 , padding = True , truncation = True,  return_tensors='pt')\n",
    "    encoded_input = encoded_input.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**encoded_input)\n",
    "    out_cpu = outputs.hidden_states[-1][:, 0, :].cpu()\n",
    "    \n",
    "    del outputs\n",
    "    \n",
    "    return out_cpu\n",
    "\n",
    "def get_LDA_sentiment_score(text , LDA_aspects, aspect_list):\n",
    "    scores = {}\n",
    "    for aspect_id, _ in LDA_aspects:\n",
    "        input_str = \"[CLS]\"+text+\" [SEP]\"+str(aspect_list[aspect_id])+\"[SEP]\"\n",
    "        inputs = absa_tokenizer(input_str, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = absa_model(**inputs)\n",
    "        scores[aspect_id] = torch.sigmoid(outputs.logits).cpu()\n",
    "        \n",
    "        del outputs\n",
    "\n",
    "    return scores\n",
    "\n",
    "def get_self_sentiment_score(text , aspect_list):\n",
    "    scores = []\n",
    "    for aspect in aspect_list:\n",
    "        input_str = \"[CLS]\"+text+\" [SEP]\"+str(aspect)+\"[SEP]\"\n",
    "        inputs = absa_tokenizer(input_str, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = absa_model(**inputs)\n",
    "        scores.append(torch.sigmoid(outputs.logits).cpu())\n",
    "        \n",
    "        del outputs\n",
    "\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "opening-relative",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:39:19.152979Z",
     "start_time": "2023-05-17T05:31:02.075080Z"
    }
   },
   "outputs": [],
   "source": [
    "all_df['LDA_aspects'] = all_df['text'].apply(lambda x : get_aspect(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "sublime-construction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T05:45:51.942605Z",
     "start_time": "2023-05-17T05:40:52.142684Z"
    }
   },
   "outputs": [],
   "source": [
    "all_df['BERT_embedding'] = all_df['text'].apply(lambda x : get_bert_embedding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "moral-angola",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T10:00:51.876946Z",
     "start_time": "2023-05-17T09:29:04.648503Z"
    }
   },
   "outputs": [],
   "source": [
    "all_df['LDA_senti'] = all_df.apply(lambda x : get_LDA_sentiment_score(x.text , x.LDA_aspects , LDA_aspect_list) , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "advance-board",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T12:09:02.974970Z",
     "start_time": "2023-05-17T10:00:53.467852Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2 hrs long\n",
    "all_df['self_senti'] = all_df.apply(lambda x : get_self_sentiment_score(x.text , restaurant_aspects) , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adequate-morocco",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T12:09:30.904025Z",
     "start_time": "2023-05-17T12:09:05.336905Z"
    }
   },
   "outputs": [],
   "source": [
    "all_df.to_pickle('../Data/restaurant_only_s_with_embedding.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9a8197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s_env",
   "language": "python",
   "name": "s_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f457e4bdb0a834eef26de468bbb8aa330d736c8b7db558705075494c4e15f1a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
